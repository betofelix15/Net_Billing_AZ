# -*- coding: utf-8 -*-
"""air_temp_utilities.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5Xgb3dvBHWa-xAHOKQ5MwcjLii9Su1B

Air temperature for all the utilities
"""

# https://developers.google.com/earth-engine/datasets/catalog/ECMWF_ERA5_MONTHLY
# ERA5 Monthly Aggregates - Latest Climate Reanalysis Produced by ECMWF / Copernicus Climate Change Service
# in Kelvins (k)
# image collection: 'ECMWF/ERA5/MONTHLY'
# citation: Copernicus Climate Change Service (C3S) (2017): ERA5: Fifth generation of ECMWF atmospheric reanalyses of the global climate. Copernicus Climate Change Service Climate Data Store (CDS), (date of access), https://cds.climate.copernicus.eu/cdsapp#!/home
# terms of use:Please acknowledge the use of ERA5 as stated in the Copernicus C3S/CAMS License agreement:
#
# 5.1.1 Where the Licensee communicates or distributes Copernicus Products to the public, the Licensee shall inform the recipients of the source by using the following or any similar notice: "Generated using Copernicus Climate Change Service information (Year)".
# 5.1.2 Where the Licensee makes or contributes to a publication or distribution containing adapted or modified Copernicus Products, the Licensee shall provide the following or any similar notice: "Contains modified Copernicus Climate Change Service information (Year)".
# 5.1.3 Any such publication or distribution covered by clauses 5.1.1 and 5.1.2 shall state that neither the European Commission nor ECMWF is responsible for any use that may be made of the Copernicus information or Data it contains.

# band : mean_2m_air_temperature , measured in Kelvins. from 223.6 to 304 K


# lets use the bound for TEP (Tucson Electric Power) in 2014, full_id: AZ_24211

# import library
import ee
import os
import geemap
import geopandas as gpd
import pandas as pd
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from tqdm import tqdm

# authenticate & initilialize earth engine
ee.Authenticate()
ee.Initialize(project='ee-jfelix-netmetering')

# set working directory/mount drive
from google.colab import drive
drive.mount('/content/drive')

#  Set working directory inside Google Drive
drive_path = '/content/drive/My Drive/net_metering'
os.makedirs(drive_path, exist_ok=True)
os.chdir(drive_path)

print("Current working directory:", os.getcwd())

# to keep console from disconnecting ctr+shift+i to inspector and go to the console
# function ClickConnect(){
#    console.log("Working");
#    document.querySelector("colab-toolbar-button#connect").click()
#}
#setInterval(ClickConnect,60000)

# get the boundary for all the offices
# you have to mount the google drive
# load the shapefile for all the utility boundaries (This was simplified in R)
all_sf = gpd.read_file('shapefiles/utility_boundaries_simplified_annual_v2.shp')

all_sf['year'].max()

# view the boundary shapefile
all_sf.head()
all_sf.tail()

# bring the data from ee and select the band
temp_collection = ee.ImageCollection('ECMWF/ERA5/MONTHLY').select('mean_2m_air_temperature')

# List of unique utility IDs
ulist = all_sf['full_id_y'].unique().tolist()

# create an empty df to collect data for loop
utility_monthly_temp = []

# Loop over each utility
for u in tqdm(ulist):  # u = "AZ_24211_2011"
    start_time = datetime.now()

    u_sf = all_sf[all_sf['full_id_y'] == u]

    # Add an explicit check for None geometry before attempting to access its attributes
    if u_sf.empty or u_sf.geometry.is_empty.any() or u_sf.geometry.values[0] is None:
        print(f"Skipping utility {u} due to empty or invalid geometry.")
        continue

    # Convert geometry to EE
    geom_json = u_sf.geometry.values[0].__geo_interface__
    boundary = ee.Geometry(geom_json)

    # Extract year
    year = int(u_sf['year'].values[0])
    d1 = datetime(year, 1, 1)
    d2 = d1 + relativedelta(months=12)

    # Filter image collection
    monthly_image = temp_collection.filterDate(d1.strftime('%Y-%m-%d'), d2.strftime('%Y-%m-%d'))

    # Function to extract average temp for each image
    def extract_temp(image):
        date = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd HH:mm')

        med_temp = image.reduceRegion(
            reducer=ee.Reducer.median(),
            geometry=boundary,
            scale=5000,
            maxPixels=1e13
        ).get('mean_2m_air_temperature')

        mean_temp = image.reduceRegion(
            reducer=ee.Reducer.mean(),
            geometry=boundary,
            scale=5000,
            maxPixels=1e13
        ).get('mean_2m_air_temperature')
        return ee.Feature(None, {'date': date, 'avg_temp': mean_temp, 'median_temp': med_temp})
    # Map over collection and get features
    temp_features = monthly_image.map(extract_temp)
    temp_table = ee.FeatureCollection(temp_features)

    # Export to client
    try:
        temp_dicts = geemap.ee_to_geojson(temp_table)
        temp_df = pd.DataFrame(temp_dicts['features'])
        temp_df['date'] = pd.to_datetime(temp_df['properties'].apply(lambda x: x['date']))
        temp_df['avg_temp'] = temp_df['properties'].apply(lambda x: x['avg_temp'])
        temp_df['median_temp'] = temp_df['properties'].apply(lambda x: x['median_temp'])
        temp_df.drop(columns=['properties', 'type', 'geometry'], inplace=True)

        # Add utility info
        temp_df['full_id_y'] = u
        temp_df['year'] = year
        temp_df['month'] = temp_df['date'].dt.month

        # save
        utility_monthly_temp.append(temp_df)

    except Exception as e:
        print(f"Failed for utility {u}: {e}")
        continue

    print(f"{u} completed in {datetime.now() - start_time}")

# Combine all data and write to CSV
result_df = pd.concat(utility_monthly_temp, ignore_index=True)

# Save to CSV
result_df.to_csv("data files/monthly_air_temp_utilities_kelvin.csv", index=False)

# read in the dataset
result_df = pd.read_csv('data files/monthly_air_temp_utilities_kelvin.csv')

# check out the data
result_df.head()

# see the values that were failed or not in the u_sf
failed_utilities = [u for u in ulist if u not in result_df['full_id_y'].unique().tolist()]
print(failed_utilities)

# find these in the all_sf
failed_df = all_sf[all_sf['full_id_y'].isin(failed_utilities)]
failed_df.head()

# use the geometry from results that worked by the same full_id
# start with the unique full_id in failed_df
fi_ulist = failed_df['full_id'].unique().tolist()

# make the full_id in results by removing everything after first number  from full_id_y
result_df['full_id'] = result_df['full_id_y'].apply(lambda x: x.rsplit('_', 1)[0])
result_df.head()

# find the full_id's from fi_ulist in result_df
good_fi_df = result_df[result_df['full_id'].isin(fi_ulist)]
good_fi_df.head()

# use the geometries from all_sf that match what is in good_fi_df as an sf
# object
good_fi_sf = all_sf[all_sf['full_id_y'].isin(good_fi_df['full_id_y'])]
good_fi_sf

# select the latest observation for full_id in good_fi_sf
good_fi_sf = good_fi_sf.sort_values('year', ascending=False).drop_duplicates('full_id')

# drop the year and full_id_y
good_fi_sf = good_fi_sf.drop(columns=['year', 'full_id_y'])
good_fi_sf

# replace the geometry in failed_df and attach the good geometry from good_fi_sf by full_id
failed_df['geometry'] = failed_df['full_id'].map(good_fi_sf.set_index('full_id')['geometry'])

# see the top 20 values
failed_df.head(20)

# make the failed_df into an sf object
failed_df = gpd.GeoDataFrame(failed_df, geometry='geometry')
failed_df.head()

# this requires a new image collection
#https://developers.google.com/earth-engine/datasets/catalog/NCEP_RE_surface_temp#bands

temp_collection = ee.ImageCollection("NCEP_RE/surface_temp").select('air')

# create an empty df to collect data for loop
utility_monthly_temp = []

# Loop over each utility
for u in tqdm(failed_utilities):  # u = "WY_14354_2023"
    start_time = datetime.now()

    u_sf = failed_df[failed_df['full_id_y'] == u]

    # Add an explicit check for None geometry before attempting to access its attributes
    if u_sf.empty or u_sf.geometry.is_empty.any() or u_sf.geometry.values[0] is None:
        print(f"Skipping utility {u} due to empty or invalid geometry.")
        continue

    # Convert geometry to EE
    geom_json = u_sf.geometry.values[0].__geo_interface__
    boundary = ee.Geometry(geom_json)

    # Extract year
    year = int(u_sf['year'].values[0])
    d1 = datetime(year, 1, 1)
    d2 = d1 + relativedelta(months=12)

    # Filter image collection
    monthly_image = temp_collection.filterDate(d1.strftime('%Y-%m-%d'), d2.strftime('%Y-%m-%d'))

    # Function to extract average temp for each image
    def extract_temp(image):
        date = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd HH:mm')

        med_temp = image.reduceRegion(
            reducer=ee.Reducer.median(),
            geometry=boundary,
            scale=5000,
            maxPixels=1e13
        ).get('air')

        mean_temp = image.reduceRegion(
            reducer=ee.Reducer.mean(),
            geometry=boundary,
            scale=5000,
            maxPixels=1e13
        ).get('air')
        return ee.Feature(None, {'date': date, 'avg_temp': mean_temp, 'median_temp': med_temp})
    # Map over collection and get features
    temp_features = monthly_image.map(extract_temp)
    temp_table = ee.FeatureCollection(temp_features)

    # Export to client
    try:
        temp_dicts = geemap.ee_to_geojson(temp_table)
        temp_df = pd.DataFrame(temp_dicts['features'])
        temp_df['date'] = pd.to_datetime(temp_df['properties'].apply(lambda x: x['date']))
        temp_df['avg_temp'] = temp_df['properties'].apply(lambda x: x['avg_temp'])
        temp_df['median_temp'] = temp_df['properties'].apply(lambda x: x['median_temp'])
        temp_df.drop(columns=['properties', 'type', 'geometry'], inplace=True)

        # Add utility info
        temp_df['full_id_y'] = u
        temp_df['year'] = year
        temp_df['month'] = temp_df['date'].dt.month

        # save
        utility_monthly_temp.append(temp_df)

    except Exception as e:
        print(f"Failed for utility {u}: {e}")
        continue

    print(f"{u} completed in {datetime.now() - start_time}")

# Combine all data and write to CSV
missing_df = pd.concat(utility_monthly_temp, ignore_index=True)

# Save to CSV
missing_df.to_csv("data files/missing_monthly_air_temp_utilities_kelvin.csv", index=False)


# read in the missing and non missing data
missing_cloud = pd.read



