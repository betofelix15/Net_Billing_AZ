# -*- coding: utf-8 -*-
"""cloud_coverage.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rtyDgzX5lxyYDFftehiwIVR4noSeecag

Collect Cloud coverage for various zip codes
"""

# import library
import ee
import os
import geemap
import geopandas as gpd
import pandas as pd
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta
from tqdm import tqdm

# authenticate & initilialize earth engine
ee.Authenticate()
ee.Initialize(project='ee-jfelix-netmetering')

# set working directory
drive_path = 'C:/Users/Jesus/Box/Energy/net_metering'
os.makedirs(drive_path, exist_ok=True)
os.chdir(drive_path)

print("Current working directory:", os.getcwd())

# to keep console from disconnecting ctr+shift+i to inspector and go to the console
# function ClickConnect(){
#    console.log("Working");
#    document.querySelector("colab-toolbar-button#connect").click()
#}
#setInterval(ClickConnect,60000)

# use dataset = ee.ImageCollection('NOAA/NCEP_DOE_RE2/total_cloud_coverage')
# band = "tcdc" (total cloud coverage in percentage: 0 to 100)
# filter dates :2011 to 2020

# get the boundary for all the offices
# you have to mount the google drive
# load the shapefile for all the utility boundaries (This was simplified in R)
all_sf = gpd.read_file('shapefiles/utility_boundaries_annual_v2.shp')

# view the boundary shapefile
all_sf.head()
all_sf.tail()

# This is the information for the google earth engine data that has cloud coverage
# https://developers.google.com/earth-engine/datasets/catalog/NOAA_NCEP_DOE_RE2_total_cloud_coverage

# description: NCEP-DOE Reanalysis 2 project is using a state-of-the-art analysis/forecast system to perform data assimilation using past data from 1979 through the previous year.

# citation: NCEP-DOE AMIP-II Reanalysis (R-2): M. Kanamitsu, W. Ebisuzaki, J. Woollen, S-K Yang, J.J. Hnilo, M. Fiorino, and G. L. Potter. 1631-1643, Nov 2002, Bulletin of the American Meteorological Society..

# availability: 1979 to 2025

# provider: NOAA

# Cadence: 6 hrs
# Pixel size: 278300 meters
# bands: tcdc , units: %, min: 0*, max: 100*, description: Total Cloud Cover
# ee snippet: ee.ImageCollection("NOAA/NCEP_DOE_RE2/total_cloud_coverage")

# bring the data from ee and select the band
tcc_collection = ee.ImageCollection('NOAA/NCEP_DOE_RE2/total_cloud_coverage').select('tcdc')

# List of unique utility IDs
ulist = all_sf['full_id_y'].unique().tolist()

# create an empty df to collect data for loop
utility_monthly_tcc = []

# Loop over each utility
for u in tqdm(ulist):  # u = "AZ_24211_2011"
    start_time = datetime.now()

    u_sf = all_sf[all_sf['full_id_y'] == u]

    # Add an explicit check for None geometry before attempting to access its attributes
    if u_sf.empty or u_sf.geometry.is_empty.any() or u_sf.geometry.values[0] is None:
        print(f"Skipping utility {u} due to empty or invalid geometry.")
        continue

    # Convert geometry to EE
    geom_json = u_sf.geometry.values[0].__geo_interface__
    boundary = ee.Geometry(geom_json)

    # Extract year
    year = int(u_sf['year'].values[0])
    d1 = datetime(year, 1, 1)
    d2 = d1 + relativedelta(months=12)

    # Filter image collection
    monthly_image = tcc_collection.filterDate(d1.strftime('%Y-%m-%d'), d2.strftime('%Y-%m-%d'))

    # Function to extract average TCC for each image
    def extract_tcc(image):
        date = ee.Date(image.get('system:time_start')).format('YYYY-MM-dd HH:mm')

        med_tcc = image.reduceRegion(
            reducer=ee.Reducer.median(),
            geometry=boundary,
            scale=5000,
            maxPixels=1e13
        ).get('tcdc')

        mean_tcc = image.reduceRegion(
            reducer=ee.Reducer.mean(),
            geometry=boundary,
            scale=5000,
            maxPixels=1e13
        ).get('tcdc')
        return ee.Feature(None, {'date': date, 'avg_tcc': mean_tcc, 'median_tcc': med_tcc})

    # Map over collection and get features
    tcc_features = monthly_image.map(extract_tcc)
    tcc_table = ee.FeatureCollection(tcc_features)

    # Export to client
    try:
        tcc_dicts = geemap.ee_to_geojson(tcc_table)
        tcc_df = pd.DataFrame(tcc_dicts['features'])
        tcc_df['date'] = pd.to_datetime(tcc_df['properties'].apply(lambda x: x['date']))
        tcc_df['avg_tcc'] = tcc_df['properties'].apply(lambda x: x['avg_tcc'])
        tcc_df['median_tcc'] = tcc_df['properties'].apply(lambda x: x['median_tcc'])
        tcc_df.drop(columns=['properties', 'type', 'geometry'], inplace=True)

        # create the hour and month and year for tcc_df
        tcc_df['hour'] = tcc_df['date'].dt.hour
        tcc_df['day'] = tcc_df['date'].dt.day
        tcc_df['month'] = tcc_df['date'].dt.month
        tcc_df['year'] = tcc_df['date'].dt.year

        # filter out midnitght, so that we only count from 6 am to 6 pm
        tcc_df = tcc_df[tcc_df['hour'] != 0 ]

        # aggregate cloud coverage at the daily level
        tcc_df = tcc_df.groupby(['year', 'month', 'day']).agg({'avg_tcc': 'mean', 'median_tcc': 'median'}).reset_index()

        # count the number of cloudy days by those with coverage over 75% on average
        tcc_df['cloudy_days'] = tcc_df['avg_tcc'] > 75

        # now aggreagate to the monthly level, number of cloudy days, avg_tcc and median_tcc
        tcc_df = tcc_df.groupby(['year', 'month']).agg({'cloudy_days': 'sum', 'avg_tcc': 'mean', 'median_tcc': 'median'}).reset_index()

        # Add utility info
        for col in u_sf.columns:
            if col != 'geometry':
                tcc_df[col] = u_sf.iloc[0][col]

        utility_monthly_tcc.append(tcc_df)

    except Exception as e:
        print(f"Failed for utility {u}: {e}")
        continue

    print(f"{u} completed in {datetime.now() - start_time}")

# Combine all data and write to CSV
result_df = pd.concat(utility_monthly_tcc, ignore_index=True)

# Save to CSV
result_df.to_csv("data/clean data/monthly_cloudy_days_utility_all_75pcnt_cov.csv", index=False)

result_df.head()

utility_monthly_tcc.tail(30)








